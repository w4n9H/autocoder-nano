import json
import os
import time
from copy import deepcopy
import xml.sax.saxutils
from typing import List, Dict, Any, Optional, Generator, Union

from rich.markdown import Markdown

from autocoder_nano.actypes import AutoCoderArgs, SourceCodeList, SingleOutputMeta
from autocoder_nano.agent.agent_base import BaseAgent
from autocoder_nano.agent.agentic_edit_types import *
from autocoder_nano.context import get_context_manager, ConversationsPruner
from autocoder_nano.core import AutoLLM, prompt, stream_chat_with_continue
from autocoder_nano.rag.token_counter import count_tokens
from autocoder_nano.utils.formatted_log_utils import save_formatted_log
from autocoder_nano.utils.git_utils import get_uncommitted_changes
from autocoder_nano.utils.printer_utils import Printer
from autocoder_nano.agent.agentic_edit_tools import (  # Import specific resolvers
    BaseToolResolver, WebSearchToolResolver, AskFollowupQuestionToolResolver,
    AttemptCompletionToolResolver
)


printer = Printer()


REPORT_TOOL_RESOLVER_MAP: Dict[Type[BaseTool], Type[BaseToolResolver]] = {
    WebSearchTool: WebSearchToolResolver,
    AskFollowupQuestionTool: AskFollowupQuestionToolResolver,
    AttemptCompletionTool: AttemptCompletionToolResolver,  # Will stop the loop anyway
}


class AgenticReport(BaseAgent):
    def __init__(
            self, args: AutoCoderArgs, llm: AutoLLM, files: SourceCodeList, history_conversation: List[Dict[str, Any]],
            conversation_config: Optional[AgenticEditConversationConfig] = None
    ):
        super().__init__(args, llm)
        self.files = files
        self.history_conversation = history_conversation
        self.current_conversations = []
        self.shadow_manager = None
        self.file_changes: Dict[str, FileChangeEntry] = {}

        # å¯¹è¯ç®¡ç†å™¨
        self.conversation_config = conversation_config
        self.conversation_manager = get_context_manager()

        # Agentic å¯¹è¯ä¿®å‰ªå™¨
        self.agentic_pruner = ConversationsPruner(args=args, llm=self.llm)

        if self.conversation_config.action == "new":
            conversation_id = self.conversation_manager.create_conversation(
                name=self.conversation_config.query or "New Conversation",
                description=self.conversation_config.query or "New Conversation")
            self.conversation_manager.set_current_conversation(conversation_id)
        if self.conversation_config.action == "resume" and self.conversation_config.conversation_id:
            self.conversation_manager.set_current_conversation(self.conversation_config.conversation_id)

    @prompt()
    def _system_prompt_role(self):
        """
        # é¢†åŸŸç ”ç©¶å‘˜ Agent

        å›¢é˜Ÿçš„å¤šé¢†åŸŸç ”ç©¶ä¸“å®¶ã€‚ä¸ä»…ç²¾é€šæŠ€æœ¯æ¶æ„çš„æ·±åº¦è°ƒç ”ï¼Œè¿˜æ“…é•¿å¸‚åœºåˆ†æï¼Œç«äº‰å¯¹æ‰‹ç ”ç©¶ï¼Œè¡Œä¸šè¶‹åŠ¿æ´å¯Ÿå’Œäº§å“å¯è¡Œæ€§åˆ†æã€‚
        é€šè¿‡å¤šRAGçŸ¥è¯†åº“ä¸é«˜çº§è”ç½‘æœç´¢ï¼Œä¸ºæŠ€æœ¯å†³ç­–ï¼Œäº§å“è§„åˆ’å’Œå•†ä¸šæˆ˜ç•¥æä¾›åŸºäºäº‹å®ä¸æ•°æ®çš„å…¨æ–¹ä½å†³ç­–æ”¯æŒã€‚

        ## æ ¸å¿ƒèŒè´£

        - æŠ€æœ¯è°ƒç ”ï¼šå¯¹æŠ€æœ¯æ ˆï¼Œæ¶æ„ï¼Œå¼€æºåº“ï¼Œç®—æ³•ï¼Œäº‘æœåŠ¡æˆ–å…·ä½“æŠ€æœ¯éš¾é¢˜è¿›è¡Œè°ƒç ”ã€‚
        - å¸‚åœºåˆ†æï¼šç ”ç©¶å¸‚åœºè§„æ¨¡ï¼Œè¶‹åŠ¿ï¼Œç”¨æˆ·å’Œå…³é”®ç©å®¶ã€‚
        - ç«äº‰åˆ†æï¼šåˆ†æç«äº‰å¯¹æ‰‹çš„äº§å“ï¼ŒæŠ€æœ¯ï¼Œä¼˜åŠ£åŠ¿ï¼Œå¸‚åœºå®šä½ï¼Œèèµ„æƒ…å†µï¼Œç”¨æˆ·è¯„ä»·å’Œæœ€æ–°åŠ¨æ€ã€‚
        - äº§å“ç ”ç©¶ï¼šè¯„ä¼°äº§å“åˆ›æ„çš„å¯è¡Œæ€§ï¼Œæ½œåœ¨ç”¨æˆ·ç—›ç‚¹ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆä»¥åŠå¸‚åœºç¼ºå£ã€‚
        - ä¿¡æ¯ç»¼åˆï¼šæç‚¼å…³é”®æ´å¯Ÿï¼Œè¯†åˆ«é£é™©ä¸æœºä¼šã€‚
        """

    @prompt()
    def _system_prompt_workflow(self):
        """
        # å·¥ä½œæµç¨‹

        1. ç›®æ ‡æ¾„æ¸…ï¼šæ¥æ”¶ç ”ç©¶ä¸»é¢˜ï¼Œå¿…è¦æ—¶ä½¿ç”¨ ask_followup_question å·¥å…·æ¾„æ¸…ç ”ç©¶èŒƒå›´ã€‚
        2. ä¿¡æ¯æ£€ç´¢ï¼šå°†ä¸»é¢˜æ‹†åˆ†ä¸º1-4ä¸ªå­æ–¹å‘ï¼Œç”Ÿæˆä¸­è‹±æ–‡æ··åˆå…³é”®è¯ï¼Œæ¯ä¸ªå­æ–¹å‘ä½¿ç”¨ web_search å·¥å…·è¿›è¡Œä¸€æ¬¡è”ç½‘æœç´¢ï¼Œç„¶åæå–å…¶ä¸­æ ¸å¿ƒä¿¡æ¯ã€‚
        3. ä¿¡æ¯éªŒè¯ï¼šäº¤å‰éªŒè¯å…³é”®ç»“è®ºï¼ŒåŒºåˆ†äº‹å®ä¸è§‚ç‚¹
        4. åˆ†æç»¼åˆï¼šä½¿ç”¨é€‚å½“åˆ†ææ¡†æ¶ï¼ˆSWOT/PESTLEç­‰ï¼‰æ•´åˆä¿¡æ¯
        5. è¾“å‡ºäº¤ä»˜ï¼šæ ¹æ®éœ€æ±‚æä¾›å®Œæ•´æŠ¥å‘Š

        ## ä¿¡æ¯æ£€ç´¢ç­–ç•¥

        - å…³é”®è¯ç”Ÿæˆï¼šä½¿ç”¨"æŠ€æœ¯æœ¯è¯­+å¯¹æ¯”/è¯„æµ‹/å®è·µ"æ¨¡å¼ï¼Œä¸­è‹±æ–‡æ··åˆ
        - æ¥æºä¼˜å…ˆçº§ï¼šå®˜æ–¹æ–‡æ¡£ï¼ŒGitHubï¼Œæƒå¨åšå®¢ï¼Œè¡Œä¸šæŠ¥å‘Šä»¥åŠè®ºæ–‡ä¼˜å…ˆ
        - æ—¶é—´ç­›é€‰ï¼šä¼˜å…ˆé€‰å– 1-2 å¹´å†…ä¿¡æ¯ï¼ŒåŸºç¡€æ€§å†…å®¹å¯æ”¾å®½æ—¶é™
        - äº¤å‰éªŒè¯ï¼šå…³é”®ç»“è®ºéœ€è‡³å°‘ä¸¤ä¸ªå¯ä¿¡æ¥æºä½è¯

        ## è¾“å‡ºè§„èŒƒ

        - å®Œæ•´æŠ¥å‘Šç»“æ„ï¼šæ‘˜è¦ï¼ŒèƒŒæ™¯ï¼Œå¸‚åœºï¼Œç«äº‰ï¼ŒæŠ€æœ¯ï¼Œå»ºè®®ï¼Œæ¥æº
        - æ ¼å¼ï¼šMarkdownï¼Œåœ¨é€‚å½“çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨Markdownè¡¨æ ¼ï¼ŒMermaidï¼ˆå¦‚æµç¨‹å›¾ã€è±¡é™å›¾ï¼‰æ¥å‘ˆç°å¤æ‚ä¿¡æ¯ã€‚
        - é•¿åº¦ï¼š500-2000å­—ï¼Œæ ¹æ®ç”¨æˆ·è¦æ±‚è°ƒæ•´

        # ç¤ºä¾‹ä¸€ï¼šæŠ€æœ¯åˆ†æç±»ä¸»é¢˜
        ä¸»é¢˜å†…å®¹ï¼š"æ¯”è¾ƒRedisä¸MongoDBåœ¨å®æ—¶æ¨èç³»ç»Ÿåœºæ™¯ä¸‹çš„æ€§èƒ½ã€æˆæœ¬ä¸é€‚ç”¨æ€§"
        ç ”ç©¶ç›®æ ‡æ¾„æ¸…ï¼šæ ¸å¿ƒæ˜¯â€œå®æ—¶æ¨èç³»ç»Ÿâ€åœºæ™¯ï¼Œè€Œéæ³›æ³›æ¯”è¾ƒä¸¤ä¸ªæ•°æ®åº“ã€‚ä¾§é‡ç‚¹æ˜¯æ€§èƒ½ï¼ˆå»¶è¿Ÿã€ååé‡ï¼‰ã€æˆæœ¬ï¼ˆå†…å­˜ vs ç¡¬ç›˜ã€è¿ç»´å¤æ‚åº¦ï¼‰å’Œåœºæ™¯é€‚ç”¨æ€§ï¼ˆæ•°æ®ç»“æ„çµæ´»æ€§ã€æ‰©å±•æ€§ï¼‰ã€‚
        å­ä¸»é¢˜æ‹†åˆ†ä¸å…³é”®è¯ç”Ÿæˆï¼š
        - æ€§èƒ½åŸºå‡†ï¼š
            a. "Redis vs MongoDB performance benchmark latency throughput"
            b. "Redis sorted sets vs MongoDB aggregation real-time ranking"
        - æ¶æ„ä¸ç”¨ä¾‹ï¼š
            a. "ä½¿ç”¨Redisåšå®æ—¶æ¨èç³»ç»Ÿ å®è·µ æ¶æ„"
            b. "MongoDB change streams real-time recommendations"
        - æˆæœ¬ä¸è¿ç»´ï¼š
            a. "Redis memory cost optimization"
            b. "MongoDB vs Redis operational complexity scaling"
        é¢„æœŸè¾“å‡ºè¦ç‚¹ï¼š
        - ç»“è®ºå…ˆè¡Œï¼š Redisåœ¨å»¶è¿Ÿæ•æ„Ÿå‹å®æ—¶è®¡ç®—ï¼ˆå¦‚å®æ—¶æ’åã€è®¡æ•°ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†æˆæœ¬ï¼ˆå†…å­˜ï¼‰è¾ƒé«˜ï¼›MongoDBæ›´é€‚åˆå¤„ç†å¤æ‚ã€æµ·é‡æ•°æ®æ¨¡å‹å’ŒæŒä¹…åŒ–å­˜å‚¨ï¼Œå…¶Change Streamsä¹Ÿèƒ½æ”¯æŒä¸€å®šå®æ—¶æ€§ã€‚
        - å¯¹æ¯”ç»´åº¦ï¼š
            a. æ•°æ®æ¨¡å‹ï¼š Redisï¼ˆé”®å€¼ã€ä¸°å¯Œæ•°æ®ç»“æ„ï¼‰ vs MongoDBï¼ˆæ–‡æ¡£æ¨¡å‹ï¼‰
            b. æ€§èƒ½ï¼š å¼•ç”¨æƒå¨åŸºå‡†æµ‹è¯•æ•°æ®ï¼Œè¯´æ˜åœ¨è¯»å†™å»¶è¿Ÿã€ååé‡ä¸Šçš„å·®å¼‚ã€‚
            c. å®æ—¶èƒ½åŠ›ï¼š Redisï¼ˆåŸç”ŸPub/Subã€Streamsï¼‰ vs MongoDBï¼ˆChange Streamsï¼‰
            d. æˆæœ¬ï¼š å†…å­˜æˆæœ¬ vs ç¡¬ç›˜æˆæœ¬ã€æ‰˜ç®¡æœåŠ¡ä»·æ ¼å¯¹æ¯”ï¼ˆå¦‚AWS ElastiCache vs DocumentDBï¼‰
            e. é€‚ç”¨åœºæ™¯ï¼š æ¨èä¸¤è€…ç»“åˆä½¿ç”¨ï¼ˆRedisåšå®æ—¶ç‰¹å¾è®¡ç®—å’Œç¼“å­˜ï¼ŒMongoDBåšä¸»æ•°æ®å­˜å‚¨ï¼‰

        # ç¤ºä¾‹äºŒï¼šäº§å“åˆ†æç±»ä¸»é¢˜
        ä¸»é¢˜å†…å®¹ï¼š"ä¸ºä¸€ä¸ªâ€˜AIé©±åŠ¨çš„ä¸€ç«™å¼ç¤¾äº¤åª’ä½“å†…å®¹ç®¡ç†ä¸å‘å¸ƒå¹³å°â€™åˆ›ä¸šæƒ³æ³•è¿›è¡Œå¸‚åœºå’Œå¯è¡Œæ€§åˆ†æ"
        ç ”ç©¶ç›®æ ‡æ¾„æ¸…ï¼šéªŒè¯è¯¥æƒ³æ³•æ˜¯å¦è§£å†³çœŸå®ç—›ç‚¹ã€å¸‚åœºè§„æ¨¡æ˜¯å¦è¶³å¤Ÿã€ç«äº‰å¯¹æ‰‹æƒ…å†µä»¥åŠæŠ€æœ¯å¯è¡Œæ€§ã€‚é‡ç‚¹è¾“å‡ºæ˜¯å¸‚åœºæœºä¼šå’Œé£é™©ã€‚
        å­ä¸»é¢˜æ‹†åˆ†ä¸å…³é”®è¯ç”Ÿæˆï¼š
        - å¸‚åœºæ ¼å±€ä¸è§„æ¨¡ï¼š
            a. "social media management platform market size"
            b. "ä¸­å›½ ç¤¾äº¤åª’ä½“ å¤šå¹³å°ç®¡ç† å·¥å…· éœ€æ±‚"
        - ç«äº‰å¯¹æ‰‹åˆ†æï¼š
            a. "Hootsuite vs Buffer features pricing"
            b. "æ–°å…´AIç¤¾äº¤å†…å®¹ç®¡ç†å¹³å°èèµ„æƒ…å†µ"
        - ç”¨æˆ·ç—›ç‚¹ä¸AIåº”ç”¨ï¼š
            a. "social media manager pain points scheduling analytics"
            b. "AI generated social media content copywriting"
        - æŠ€æœ¯å¯è¡Œæ€§ï¼š
            a. "ç¤¾äº¤åª’ä½“APIé›†æˆéš¾åº¦ Instagram Twitter Meta developer"
            b. "AIGCå†…å®¹ç”Ÿæˆ API æˆæœ¬ åˆè§„æ€§"
        é¢„æœŸè¾“å‡ºè¦ç‚¹ï¼š
        - æ‘˜è¦ï¼šå¸‚åœºå·¨å¤§ä½†ç«äº‰æ¿€çƒˆ
        - å¸‚åœºåˆ†æï¼šå¼•ç”¨æŠ¥å‘Šè¯´æ˜SaaSç±»è¥é”€å·¥å…·çš„å¸‚åœºè§„æ¨¡å’Œå¢é•¿ç‡ã€‚
        - ç«äº‰åˆ†æï¼šç”¨è¡¨æ ¼å¯¹æ¯”ä¸»è¦ç«å“ï¼ˆå¦‚Hootsuite, Buffer, Sprout Socialï¼‰çš„åŠŸèƒ½ã€å®šä»·ã€ä¼˜åŠ£åŠ¿
        - ç”¨æˆ·åˆ†æï¼šç›®æ ‡ç”¨æˆ·æ˜¯ä¸­å°ä¼ä¸šçš„è¥é”€äººå‘˜ã€ç½‘çº¢ç­‰
        - æŠ€æœ¯å¯è¡Œæ€§ï¼šæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå„ç¤¾äº¤åª’ä½“APIçš„ç¨³å®šæ€§å’Œé™åˆ¶ï¼ˆå¦‚æ¯æ—¥å‘å¸ƒä¸Šé™ï¼‰ã€AIGCAPIçš„æˆæœ¬ä¸ç”Ÿæˆè´¨é‡ã€ä»¥åŠæ•°æ®éšç§åˆè§„é—®é¢˜ã€‚
        - é£é™©ä¸å»ºè®®ï¼š
        """

    @prompt()
    def _system_prompt_tools(self):
        """
        # å·¥å…·ä½¿ç”¨è¯´æ˜

        1. ä½ å¯ä½¿ç”¨ä¸€ç³»åˆ—å·¥å…·ï¼Œéƒ¨åˆ†å·¥å…·éœ€ç»ç”¨æˆ·æ‰¹å‡†æ‰èƒ½æ‰§è¡Œã€‚
        2. æ¯æ¡æ¶ˆæ¯ä¸­ä»…èƒ½ä½¿ç”¨ä¸€ä¸ªå·¥å…·ï¼Œç”¨æˆ·å›å¤ä¸­ä¼šåŒ…å«è¯¥å·¥å…·çš„æ‰§è¡Œç»“æœã€‚
        3. ä½ è¦å€ŸåŠ©å·¥å…·é€æ­¥å®Œæˆç»™å®šä»»åŠ¡ï¼Œæ¯ä¸ªå·¥å…·çš„ä½¿ç”¨éƒ½éœ€ä¾æ®å‰ä¸€ä¸ªå·¥å…·çš„ä½¿ç”¨ç»“æœã€‚

        # å·¥å…·ä½¿ç”¨æ ¼å¼

        å·¥å…·ä½¿ç”¨é‡‡ç”¨ XML é£æ ¼æ ‡ç­¾è¿›è¡Œæ ¼å¼åŒ–ã€‚å·¥å…·åç§°åŒ…å«åœ¨å¼€å§‹å’Œç»“æŸæ ‡ç­¾å†…ï¼Œæ¯ä¸ªå‚æ•°åŒæ ·åŒ…å«åœ¨å„è‡ªçš„æ ‡ç­¾ä¸­ã€‚å…¶ç»“æ„å¦‚ä¸‹ï¼š
        <tool_name>
        <parameter1_name>value1</parameter1_name>
        <parameter2_name>value2</parameter2_name>
        ...
        </tool_name>
        ä¾‹å¦‚ï¼š
        <read_file>
        <path>src/main.js</path>
        </read_file>

        ä¸€å®šè¦ä¸¥æ ¼éµå¾ªæ­¤å·¥å…·ä½¿ç”¨æ ¼å¼ï¼Œä»¥ç¡®ä¿æ­£ç¡®è§£æå’Œæ‰§è¡Œã€‚

        # å·¥å…·åˆ—è¡¨

        ## web_searchï¼ˆè”ç½‘æ£€ç´¢ï¼‰
        æè¿°ï¼š
        - é€šè¿‡æœç´¢å¼•æ“åœ¨äº’è”ç½‘ä¸Šæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œæ”¯æŒå…³é”®è¯æœç´¢ã€‚
        å‚æ•°ï¼š
        - queryï¼ˆå¿…å¡«ï¼‰ï¼šè¦æœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­
        ç”¨æ³•è¯´æ˜ï¼š
        <web_search>
        <query>Search keywords here</query>
        </web_search>
        ç”¨æ³•ç¤ºä¾‹ï¼š
        åœºæ™¯ä¸€ï¼šåŸºç¡€å…³é”®è¯æœç´¢
        ç›®æ ‡ï¼šæŸ¥æ‰¾å…³äºç¥ç»ç½‘ç»œçš„ç ”ç©¶è¿›å±•ã€‚
        æ€ç»´è¿‡ç¨‹ï¼šé€šè¿‡ä¸€äº›å…³é”®è¯ï¼Œæ¥è·å–æœ‰å…³äºç¥ç»ç½‘ç»œå­¦æœ¯ä¿¡æ¯
        <web_search>
        <query>neural network research advances</query>
        </web_search>
        åœºæ™¯äºŒï¼šç®€å•çŸ­è¯­æœç´¢
        ç›®æ ‡ï¼šæŸ¥æ‰¾å…³äºé‡å­è®¡ç®—çš„è¯¦ç»†ä»‹ç»ã€‚
        æ€ç»´è¿‡ç¨‹ï¼šé€šè¿‡ä¸€ä¸ªçŸ­è¯­ï¼Œæ¥è·å–æœ‰å…³äºé‡å­è®¡ç®—çš„ä¿¡æ¯
        <web_search>
        <query>é‡å­è®¡ç®—çš„è¯¦ç»†ä»‹ç»</query>
        </web_search>

        ## ask_followup_questionï¼ˆæå‡ºåç»­é—®é¢˜ï¼‰
        æè¿°ï¼š
        - å‘ç”¨æˆ·æé—®è·å–ä»»åŠ¡æ‰€éœ€ä¿¡æ¯ã€‚
        - å½“é‡åˆ°æ­§ä¹‰ï¼Œéœ€è¦æ¾„æ¸…æˆ–éœ€è¦æ›´å¤šç»†èŠ‚ä»¥æœ‰æ•ˆæ¨è¿›æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
        - å®ƒé€šè¿‡ä¸ç”¨æˆ·ç›´æ¥æ²Ÿé€šå®ç°äº¤äº’å¼é—®é¢˜è§£å†³ï¼Œåº”æ˜æ™ºä½¿ç”¨ï¼Œä»¥åœ¨æ”¶é›†å¿…è¦ä¿¡æ¯å’Œé¿å…è¿‡å¤šæ¥å›æ²Ÿé€šä¹‹é—´å–å¾—å¹³è¡¡ã€‚
        å‚æ•°ï¼š
        - questionï¼ˆå¿…å¡«ï¼‰ï¼šæ¸…æ™°å…·ä½“çš„é—®é¢˜ã€‚
        - optionsï¼ˆå¯é€‰ï¼‰ï¼š2-5ä¸ªé€‰é¡¹çš„æ•°ç»„ï¼Œæ¯ä¸ªé€‰é¡¹åº”ä¸ºæè¿°å¯èƒ½ç­”æ¡ˆçš„å­—ç¬¦ä¸²ï¼Œå¹¶éæ€»æ˜¯éœ€è¦æä¾›é€‰é¡¹ï¼Œå°‘æ•°æƒ…å†µä¸‹æœ‰åŠ©äºé¿å…ç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ã€‚
        ç”¨æ³•è¯´æ˜ï¼š
        <ask_followup_question>
        <question>Your question here</question>
        <options>
        Array of options here (optional), e.g. ["Option 1", "Option 2", "Option 3"]
        </options>
        </ask_followup_question>
        ç”¨æ³•ç¤ºä¾‹ï¼š
        åœºæ™¯ä¸€ï¼šæ¾„æ¸…éœ€æ±‚
        ç›®æ ‡ï¼šç”¨æˆ·åªè¯´è¦ä¿®æ”¹æ–‡ä»¶ï¼Œä½†æ²¡æœ‰æä¾›æ–‡ä»¶åã€‚
        æ€ç»´è¿‡ç¨‹ï¼šéœ€è¦å‘ç”¨æˆ·è¯¢é—®å…·ä½“è¦ä¿®æ”¹å“ªä¸ªæ–‡ä»¶ï¼Œæä¾›é€‰é¡¹å¯ä»¥æé«˜æ•ˆç‡ã€‚
        <ask_followup_question>
        <question>è¯·é—®æ‚¨è¦ä¿®æ”¹å“ªä¸ªæ–‡ä»¶ï¼Ÿ</question>
        <options>
        ["src/app.js", "src/index.js", "package.json"]
        </options>
        </ask_followup_question>
        åœºæ™¯äºŒï¼šè¯¢é—®ç”¨æˆ·åå¥½
        ç›®æ ‡ï¼šåœ¨å®ç°æ–°åŠŸèƒ½æ—¶ï¼Œæœ‰å¤šç§æŠ€æœ¯æ–¹æ¡ˆå¯ä¾›é€‰æ‹©ã€‚
        æ€ç»´è¿‡ç¨‹ï¼šä¸ºäº†ç¡®ä¿æœ€ç»ˆå®ç°ç¬¦åˆç”¨æˆ·é¢„æœŸï¼Œéœ€è¦è¯¢é—®ç”¨æˆ·æ›´å€¾å‘äºå“ªç§æ–¹æ¡ˆã€‚
        <ask_followup_question>
        <question>æ‚¨å¸Œæœ›ä½¿ç”¨å“ªä¸ªæ¡†æ¶æ¥å®ç°å‰ç«¯ç•Œé¢ï¼Ÿ</question>
        <options>
        ["React", "Vue", "Angular"]
        </options>
        </ask_followup_question>

        ## attempt_completionï¼ˆå°è¯•å®Œæˆä»»åŠ¡ï¼‰
        æè¿°ï¼š
        - æ¯æ¬¡å·¥å…·ä½¿ç”¨åï¼Œç”¨æˆ·ä¼šå›å¤è¯¥å·¥å…·ä½¿ç”¨çš„ç»“æœï¼Œå³æ˜¯å¦æˆåŠŸä»¥åŠå¤±è´¥åŸå› ï¼ˆå¦‚æœ‰ï¼‰ã€‚
        - ä¸€æ—¦æ”¶åˆ°å·¥å…·ä½¿ç”¨ç»“æœå¹¶ç¡®è®¤ä»»åŠ¡å®Œæˆï¼Œä½¿ç”¨æ­¤å·¥å…·å‘ç”¨æˆ·å±•ç¤ºå·¥ä½œæˆæœã€‚
        - å¯é€‰åœ°ï¼Œä½ å¯ä»¥æä¾›ä¸€ä¸ª CLI å‘½ä»¤æ¥å±•ç¤ºå·¥ä½œæˆæœã€‚ç”¨æˆ·å¯èƒ½ä¼šæä¾›åé¦ˆï¼Œä½ å¯æ®æ­¤è¿›è¡Œæ”¹è¿›å¹¶å†æ¬¡å°è¯•ã€‚
        é‡è¦æç¤ºï¼š
        - åœ¨ç¡®è®¤ç”¨æˆ·å·²ç¡®è®¤ä¹‹å‰çš„å·¥å…·ä½¿ç”¨æˆåŠŸä¹‹å‰ï¼Œä¸å¾—ä½¿ç”¨æ­¤å·¥å…·ã€‚å¦åˆ™å°†å¯¼è‡´ä»£ç æŸåå’Œç³»ç»Ÿæ•…éšœã€‚
        - åœ¨ä½¿ç”¨æ­¤å·¥å…·ä¹‹å‰ï¼Œå¿…é¡»åœ¨<thinking></thinking>æ ‡ç­¾ä¸­è‡ªé—®æ˜¯å¦å·²ä»ç”¨æˆ·å¤„ç¡®è®¤ä¹‹å‰çš„å·¥å…·ä½¿ç”¨æˆåŠŸã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä¸è¦ä½¿ç”¨æ­¤å·¥å…·ã€‚
        å‚æ•°ï¼š
        - resultï¼ˆå¿…å¡«ï¼‰ï¼šä»»åŠ¡çš„ç»“æœï¼Œåº”ä»¥æœ€ç»ˆå½¢å¼è¡¨è¿°ï¼Œæ— éœ€ç”¨æˆ·è¿›ä¸€æ­¥è¾“å…¥ï¼Œä¸å¾—åœ¨ç»“æœç»“å°¾æå‡ºé—®é¢˜æˆ–æä¾›è¿›ä¸€æ­¥å¸®åŠ©ã€‚
        - commandï¼ˆå¯é€‰ï¼‰ï¼šç”¨äºå‘ç”¨æˆ·æ¼”ç¤ºç»“æœçš„ CLI å‘½ä»¤ã€‚
        ç”¨æ³•è¯´æ˜ï¼š
        <attempt_completion>
        <result>
        Your final result description here
        </result>
        <command>Command to demonstrate result (optional)</command>
        </attempt_completion>
        ç”¨æ³•ç¤ºä¾‹ï¼š
        åœºæ™¯ä¸€ï¼šè¾“å‡ºç»¼åˆæ€§ç ”ç©¶æŠ¥å‘Šå†…å®¹
        ç›®æ ‡ï¼šå‘ç”¨æˆ·å±•ç¤ºç»¼åˆæ€§ç ”ç©¶æŠ¥å‘Šå†…å®¹ã€‚
        æ€ç»´è¿‡ç¨‹ï¼šæ‰€æœ‰æŸ¥è¯¢æ£€ç´¢å·¥ä½œéƒ½å·²å®Œæˆï¼Œé€šè¿‡éªŒè¯ï¼Œåˆ†æï¼Œç°åœ¨å‘ç”¨æˆ·å±•ç¤ºç»¼åˆæ€§ç ”ç©¶æŠ¥å‘Šå†…å®¹ã€‚
        <attempt_completion>
        <result>
        ç»¼åˆæ€§ç ”ç©¶æŠ¥å‘Šå…·ä½“å†…å®¹
        </result>
        </attempt_completion>

        # é”™è¯¯å¤„ç†
        - å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œä½ éœ€è¦åˆ†æé”™è¯¯ä¿¡æ¯ï¼Œå¹¶é‡æ–°å°è¯•ï¼Œæˆ–è€…å‘ç”¨æˆ·æŠ¥å‘Šé”™è¯¯å¹¶è¯·æ±‚å¸®åŠ©ï¼ˆä½¿ç”¨ ask_followup_question å·¥å…·ï¼‰

        ## å·¥å…·ç†”æ–­æœºåˆ¶
        - å·¥å…·è¿ç»­å¤±è´¥2æ¬¡æ—¶å¯åŠ¨å¤‡é€‰æ–¹æ¡ˆ
        - è‡ªåŠ¨æ ‡æ³¨è¡Œä¸šæƒ¯ä¾‹æ–¹æ¡ˆä¾›ç”¨æˆ·ç¡®è®¤
        """

    @prompt()
    def _system_prompt_rules(self):
        """
        # çº¦æŸä¸æ ¸å¿ƒè§„åˆ™

        - ä¸»é¢˜åŠç›®æ ‡è¦æ˜ç¡®ï¼Œå¿…è¦æ—¶ä¸ç”¨æˆ·æ²Ÿé€šç¡®è®¤ã€‚
        - æ¯æ¬¡ç ”ç©¶å¯ä»¥ä½¿ç”¨ 1-4 æ¬¡ web_search æœç´¢å·¥å…·ã€‚
        - è¾“å‡ºå‰ç¡®ä¿ä¿¡æ¯ç»è¿‡éªŒè¯ã€‚
        - æŠ¥å‘Šæ ¼å¼ä¸º Markdownï¼Œå†…å®¹å°½é‡ç²¾ç®€ï¼Œå°½é‡ä¿æŒåœ¨500-2000å­—ä¹‹é—´ã€‚
        - æœ€åä½¿ç”¨ attempt_completion å·¥å…·è¾“å‡ºç»¼åˆæŠ¥å‘Šã€‚
        """

    def analyze(self, request: AgenticEditRequest) -> (
            Generator)[Union[LLMOutputEvent, LLMThinkingEvent, ToolCallEvent, ToolResultEvent, CompletionEvent,
                             ErrorEvent, WindowLengthChangeEvent, TokenUsageEvent,
                             PlanModeRespondEvent] | None, None, None]:
        conversations = [
            {"role": "system", "content": self._system_prompt_role.prompt()},
            {"role": "system", "content": self._system_prompt_workflow.prompt()},
            {"role": "system", "content": self._system_prompt_tools.prompt()},
            {"role": "system", "content": self._system_prompt_rules.prompt()}
        ]

        printer.print_text(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯é•¿åº¦(token): {count_tokens(json.dumps(conversations, ensure_ascii=False))}",
                           style="green")

        if self.conversation_config.action == "resume":
            current_conversation = self.conversation_manager.get_current_conversation()
            # å¦‚æœç»§ç»­çš„æ˜¯å½“å‰çš„å¯¹è¯ï¼Œå°†å…¶æ¶ˆæ¯åŠ å…¥åˆ° conversations ä¸­
            if current_conversation and current_conversation.get('messages'):
                for message in current_conversation['messages']:
                    # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®ï¼ˆåŒ…å« role å’Œ content å­—æ®µï¼‰
                    if isinstance(message, dict) and 'role' in message and 'content' in message:
                        conversations.append({
                            "role": message['role'],
                            "content": message['content']
                        })
                printer.print_text(f"ğŸ“‚ æ¢å¤å¯¹è¯ï¼Œå·²æœ‰ {len(current_conversation['messages'])} æ¡ç°æœ‰æ¶ˆæ¯", style="green")
        if self.conversation_manager.get_current_conversation_id() is None:
            conv_id = self.conversation_manager.create_conversation(name=self.conversation_config.query,
                                                                    description=self.conversation_config.query)
            self.conversation_manager.set_current_conversation(conv_id)

        self.conversation_manager.set_current_conversation(self.conversation_manager.get_current_conversation_id())

        conversations.append({
            "role": "user", "content": request.user_input
        })

        self.conversation_manager.append_message_to_current(
            role="user",
            content=request.user_input,
            metadata={})

        self.current_conversations = conversations

        # è®¡ç®—åˆå§‹å¯¹è¯çª—å£é•¿åº¦å¹¶è§¦å‘äº‹ä»¶
        conversation_str = json.dumps(conversations, ensure_ascii=False)
        current_tokens = count_tokens(conversation_str)
        yield WindowLengthChangeEvent(tokens_used=current_tokens)

        iteration_count = 0
        tool_executed = False
        should_yield_completion_event = False
        completion_event = None

        while True:
            iteration_count += 1
            if iteration_count % 20 == 0:
                conversations.append({"role": "user", "content": self._system_prompt_rules.prompt()})  # å¼ºåŒ–è§„åˆ™è®°å¿†
            tool_executed = False
            last_message = conversations[-1]
            printer.print_text(f"ğŸ”„ å½“å‰ä¸ºç¬¬ {iteration_count} è½®å¯¹è¯, å†å²ä¼šè¯é•¿åº¦(Context):{len(conversations)}",
                               style="green")

            if last_message["role"] == "assistant":
                if should_yield_completion_event:
                    if completion_event is None:
                        yield CompletionEvent(completion=AttemptCompletionTool(
                            result=last_message["content"],
                            command=""
                        ), completion_xml="")
                    else:
                        yield completion_event
                break

            assistant_buffer = ""

            # å®é™…è¯·æ±‚å¤§æ¨¡å‹
            llm_response_gen = stream_chat_with_continue(
                llm=self.llm,
                conversations=self.agentic_pruner.prune_conversations(deepcopy(conversations)),
                llm_config={},  # Placeholder for future LLM configs
                args=self.args
            )

            parsed_events = self.stream_and_parse_llm_response(llm_response_gen)

            event_count = 0
            mark_event_should_finish = False
            for event in parsed_events:
                event_count += 1

                if mark_event_should_finish:
                    if isinstance(event, TokenUsageEvent):
                        yield event
                    continue

                if isinstance(event, (LLMOutputEvent, LLMThinkingEvent)):
                    assistant_buffer += event.text
                    yield event  # Yield text/thinking immediately for display

                elif isinstance(event, ToolCallEvent):
                    tool_executed = True
                    tool_obj = event.tool
                    tool_name = type(tool_obj).__name__
                    tool_xml = event.tool_xml  # Already reconstructed by parser

                    # Append assistant's thoughts and the tool call to history
                    printer.print_panel(content=f"tool_xml \n{tool_xml}", title=f"ğŸ› ï¸ å·¥å…·è§¦å‘: {tool_name}",
                                        center=True)

                    # è®°å½•å½“å‰å¯¹è¯çš„tokenæ•°é‡
                    conversations.append({
                        "role": "assistant",
                        "content": assistant_buffer + tool_xml
                    })
                    self.conversation_manager.append_message_to_current(
                        role="assistant",
                        content=assistant_buffer + tool_xml,
                        metadata={})
                    assistant_buffer = ""  # Reset buffer after tool call

                    # è®¡ç®—å½“å‰å¯¹è¯çš„æ€» token æ•°é‡å¹¶è§¦å‘äº‹ä»¶
                    current_conversation_str = json.dumps(conversations, ensure_ascii=False)
                    total_tokens = count_tokens(current_conversation_str)
                    yield WindowLengthChangeEvent(tokens_used=total_tokens)

                    yield event  # Yield the ToolCallEvent for display

                    # Handle AttemptCompletion separately as it ends the loop
                    if isinstance(tool_obj, AttemptCompletionTool):
                        printer.print_panel(content=f"å®Œæˆç»“æœ: {tool_obj.result[:50]}...",
                                            title="AttemptCompletionToolï¼Œæ­£åœ¨ç»“æŸä¼šè¯", center=True)
                        completion_event = CompletionEvent(completion=tool_obj, completion_xml=tool_xml)
                        # save_formatted_log(self.args.source_dir, json.dumps(conversations, ensure_ascii=False),
                        #                    "agentic_conversation")
                        mark_event_should_finish = True
                        should_yield_completion_event = True
                        continue

                    if isinstance(tool_obj, PlanModeRespondTool):
                        printer.print_panel(content=f"Plan æ¨¡å¼å“åº”å†…å®¹: {tool_obj.response[:50]}...",
                                            title="PlanModeRespondToolï¼Œæ­£åœ¨ç»“æŸä¼šè¯", center=True)
                        yield PlanModeRespondEvent(completion=tool_obj, completion_xml=tool_xml)
                        # save_formatted_log(self.args.source_dir, json.dumps(conversations, ensure_ascii=False),
                        #                    "agentic_conversation")
                        mark_event_should_finish = True
                        continue

                    # Resolve the tool
                    resolver_cls = REPORT_TOOL_RESOLVER_MAP.get(type(tool_obj))
                    if not resolver_cls:
                        tool_result = ToolResult(
                            success=False, message="é”™è¯¯ï¼šå·¥å…·è§£æå™¨æœªå®ç°.", content=None)
                        result_event = ToolResultEvent(tool_name=type(tool_obj).__name__, result=tool_result)
                        error_xml = (f"<tool_result tool_name='{type(tool_obj).__name__}' success='false'>"
                                     f"<message>Error: Tool resolver not implemented.</message>"
                                     f"<content></content></tool_result>")
                    else:
                        try:
                            resolver = resolver_cls(agent=self, tool=tool_obj, args=self.args)
                            tool_result: ToolResult = resolver.resolve()
                            result_event = ToolResultEvent(tool_name=type(tool_obj).__name__, result=tool_result)

                            # Prepare XML for conversation history
                            escaped_message = xml.sax.saxutils.escape(tool_result.message)
                            content_str = str(
                                tool_result.content) if tool_result.content is not None else ""
                            escaped_content = xml.sax.saxutils.escape(
                                content_str)
                            error_xml = (
                                f"<tool_result tool_name='{type(tool_obj).__name__}' success='{str(tool_result.success).lower()}'>"
                                f"<message>{escaped_message}</message>"
                                f"<content>{escaped_content}</content>"
                                f"</tool_result>"
                            )
                        except Exception as e:
                            error_message = f"Critical Error during tool execution: {e}"
                            tool_result = ToolResult(success=False, message=error_message, content=None)
                            result_event = ToolResultEvent(tool_name=type(tool_obj).__name__, result=tool_result)
                            escaped_error = xml.sax.saxutils.escape(error_message)
                            error_xml = (f"<tool_result tool_name='{type(tool_obj).__name__}' success='false'>"
                                         f"<message>{escaped_error}</message>"
                                         f"<content></content></tool_result>")

                    yield result_event  # Yield the ToolResultEvent for display

                    # æ·»åŠ å·¥å…·ç»“æœåˆ°å¯¹è¯å†å²
                    conversations.append({
                        "role": "user",  # Simulating the user providing the tool result
                        "content": error_xml
                    })
                    self.conversation_manager.append_message_to_current(
                        role="user",
                        content=error_xml,
                        metadata={})

                    # è®¡ç®—å½“å‰å¯¹è¯çš„æ€» token æ•°é‡å¹¶è§¦å‘äº‹ä»¶
                    current_conversation_str = json.dumps(conversations, ensure_ascii=False)
                    total_tokens = count_tokens(current_conversation_str)
                    yield WindowLengthChangeEvent(tokens_used=total_tokens)

                    # ä¸€æ¬¡äº¤äº’åªèƒ½æœ‰ä¸€æ¬¡å·¥å…·ï¼Œå‰©ä¸‹çš„å…¶å®å°±æ²¡æœ‰ç”¨äº†ï¼Œä½†æ˜¯å¦‚æœä¸è®©æµå¼å¤„ç†å®Œï¼Œæˆ‘ä»¬å°±æ— æ³•è·å–æœåŠ¡ç«¯
                    # è¿”å›çš„tokenæ¶ˆè€—å’Œè®¡è´¹ï¼Œæ‰€ä»¥é€šè¿‡æ­¤æ ‡è®°æ¥å®Œæˆè¿›å…¥ç©ºè½¬ï¼Œç›´åˆ°æµå¼èµ°å®Œï¼Œè·å–åˆ°æœ€åçš„tokenæ¶ˆè€—å’Œè®¡è´¹
                    mark_event_should_finish = True

                elif isinstance(event, ErrorEvent):
                    yield event
                elif isinstance(event, TokenUsageEvent):
                    yield event

            if not tool_executed:
                # No tool executed in this LLM response cycle
                printer.print_text("LLMå“åº”å®Œæˆ, æœªæ‰§è¡Œä»»ä½•å·¥å…·", style="yellow")
                if assistant_buffer:
                    printer.print_text(f"å°† Assistant Buffer å†…å®¹å†™å…¥ä¼šè¯å†å²ï¼ˆå­—ç¬¦æ•°ï¼š{len(assistant_buffer)}ï¼‰")

                    last_message = conversations[-1]
                    if last_message["role"] != "assistant":
                        printer.print_text("æ·»åŠ æ–°çš„ Assistant æ¶ˆæ¯", style="green")
                        conversations.append({"role": "assistant", "content": assistant_buffer})
                        self.conversation_manager.append_message_to_current(
                            role="assistant", content=assistant_buffer, metadata={})
                    elif last_message["role"] == "assistant":
                        printer.print_text("è¿½åŠ å·²å­˜åœ¨çš„ Assistant æ¶ˆæ¯")
                        last_message["content"] += assistant_buffer

                    # è®¡ç®—å½“å‰å¯¹è¯çš„æ€» token æ•°é‡å¹¶è§¦å‘äº‹ä»¶
                    current_conversation_str = json.dumps(conversations, ensure_ascii=False)
                    total_tokens = count_tokens(current_conversation_str)
                    yield WindowLengthChangeEvent(tokens_used=total_tokens)

                # æ·»åŠ ç³»ç»Ÿæç¤ºï¼Œè¦æ±‚LLMå¿…é¡»ä½¿ç”¨å·¥å…·æˆ–æ˜ç¡®ç»“æŸï¼Œè€Œä¸æ˜¯ç›´æ¥é€€å‡º
                printer.print_text("ğŸ’¡ æ­£åœ¨æ·»åŠ ç³»ç»Ÿæç¤º: è¯·ä½¿ç”¨å·¥å…·æˆ–å°è¯•ç›´æ¥ç”Ÿæˆç»“æœ", style="green")

                conversations.append({
                    "role": "user",
                    "content": "æ³¨æ„ï¼šæ‚¨å¿…é¡»ä½¿ç”¨é€‚å½“çš„å·¥å…·æˆ–æ˜ç¡®å®Œæˆä»»åŠ¡ï¼ˆä½¿ç”¨ attempt_completionï¼‰ã€‚"
                               "ä¸è¦åœ¨ä¸é‡‡å–å…·ä½“è¡ŒåŠ¨çš„æƒ…å†µä¸‹æä¾›æ–‡æœ¬å›å¤ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„å·¥å…·ç»§ç»­æ“ä½œã€‚"
                })
                self.conversation_manager.append_message_to_current(
                    role="user",
                    content="æ³¨æ„ï¼šæ‚¨å¿…é¡»ä½¿ç”¨é€‚å½“çš„å·¥å…·æˆ–æ˜ç¡®å®Œæˆä»»åŠ¡ï¼ˆä½¿ç”¨ attempt_completionï¼‰ã€‚"
                            "ä¸è¦åœ¨ä¸é‡‡å–å…·ä½“è¡ŒåŠ¨çš„æƒ…å†µä¸‹æä¾›æ–‡æœ¬å›å¤ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„å·¥å…·ç»§ç»­æ“ä½œã€‚",
                    metadata={})

                # è®¡ç®—å½“å‰å¯¹è¯çš„æ€» token æ•°é‡å¹¶è§¦å‘äº‹ä»¶
                current_conversation_str = json.dumps(conversations, ensure_ascii=False)
                total_tokens = count_tokens(current_conversation_str)
                yield WindowLengthChangeEvent(tokens_used=total_tokens)
                # ç»§ç»­å¾ªç¯ï¼Œè®© LLM å†æ€è€ƒï¼Œè€Œä¸æ˜¯ break
                printer.print_text("ğŸ”„ æŒç»­è¿è¡Œ LLM äº¤äº’å¾ªç¯ï¼ˆä¿æŒä¸ä¸­æ–­ï¼‰", style="green")
                continue

        printer.print_text(f"âœ… AgenticEdit åˆ†æå¾ªç¯å·²å®Œæˆï¼Œå…±æ‰§è¡Œ {iteration_count} æ¬¡è¿­ä»£.")
        save_formatted_log(self.args.source_dir, json.dumps(conversations, ensure_ascii=False),
                           "agentic_report_conversation")

    def apply_pre_changes(self):
        uncommitted_changes = get_uncommitted_changes(self.args.source_dir)
        if uncommitted_changes != "No uncommitted changes found.":
            raise Exception("ä»£ç ä¸­åŒ…å«æœªæäº¤çš„æ›´æ–°,è¯·æ‰§è¡Œ/commit")

    def run_in_terminal(self, request: AgenticEditRequest):
        project_name = os.path.basename(os.path.abspath(self.args.source_dir))

        printer.print_text(f"ğŸš€ Agentic Report å¼€å§‹è¿è¡Œ, é¡¹ç›®å: {project_name}, ç”¨æˆ·ç›®æ ‡: {request.user_input}")

        # ç”¨äºç´¯è®¡TokenUsageEventæ•°æ®
        accumulated_token_usage = {
            "model_name": "",
            "input_tokens": 0,
            "output_tokens": 0,
        }

        try:
            self.apply_pre_changes()  # åœ¨å¼€å§‹ Agentic Report ä¹‹å‰å…ˆåˆ¤æ–­æ˜¯å¦æœ‰æœªæäº¤å˜æ›´,æœ‰å˜æ›´åˆ™ç›´æ¥é€€å‡º
            event_stream = self.analyze(request)
            for event in event_stream:
                if isinstance(event, TokenUsageEvent):
                    last_meta: SingleOutputMeta = event.usage

                    # ç´¯è®¡tokenä½¿ç”¨æƒ…å†µ
                    accumulated_token_usage["model_name"] = self.args.chat_model
                    accumulated_token_usage["input_tokens"] += last_meta.input_tokens_count
                    accumulated_token_usage["output_tokens"] += last_meta.generated_tokens_count

                    printer.print_text(f"ğŸ“ Token ä½¿ç”¨: "
                                       f"Input({last_meta.input_tokens_count})/"
                                       f"Output({last_meta.generated_tokens_count})",
                                       style="green")

                elif isinstance(event, WindowLengthChangeEvent):
                    printer.print_text(f"ğŸ“ å½“å‰ Token æ€»ç”¨é‡: {event.tokens_used}", style="green")

                elif isinstance(event, LLMThinkingEvent):
                    # ä»¥ä¸å¤ªæ˜¾çœ¼çš„æ ·å¼ï¼ˆæ¯”å¦‚ç°è‰²ï¼‰å‘ˆç°æ€è€ƒå†…å®¹
                    think_text = f"[grey]{event.text}[/grey]"
                    printer.print_panel(content=think_text, title="ğŸ’­ LLM Thinking", center=True)

                elif isinstance(event, LLMOutputEvent):
                    printer.print_panel(content=f"{event.text}", title="ğŸ’¬ LLM Output", center=True)

                elif isinstance(event, ToolCallEvent):
                    # ä¸æ˜¾ç¤º AttemptCompletionTool ç»“æœ
                    if isinstance(event.tool, AttemptCompletionTool):
                        continue

                    tool_name = type(event.tool).__name__
                    # Use the new internationalized display function
                    display_content = self.get_tool_display_message(event.tool)
                    printer.print_panel(content=display_content, title=f"ğŸ› ï¸ å·¥å…·è°ƒç”¨: {tool_name}", center=True)

                elif isinstance(event, ToolResultEvent):
                    # ä¸æ˜¾ç¤º AttemptCompletionTool å’Œ PlanModeRespondTool ç»“æœ
                    if event.tool_name == "AttemptCompletionTool":
                        continue
                    if event.tool_name == "PlanModeRespondTool":
                        continue

                    result = event.result
                    title = f"âœ… å·¥å…·è¿”å›: {event.tool_name}" if result.success else f"âŒ å·¥å…·è¿”å›: {event.tool_name}"
                    border_style = "green" if result.success else "red"
                    base_content = f"çŠ¶æ€: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}\n"
                    base_content += f"ä¿¡æ¯: {result.message}\n"

                    def _format_content(_content):
                        if len(_content) > 500:
                            return f"{_content[:200]}\n\n\n......\n\n\n{_content[-200:]}"
                        else:
                            return _content

                    # Prepare panel for base info first
                    panel_content = [base_content]
                    # syntax_content = None
                    content_str = ""
                    lexer = "python"  # Default guess

                    if result.content is not None:
                        try:
                            if isinstance(result.content, (dict, list)):
                                content_str = _format_content(json.dumps(result.content, indent=2, ensure_ascii=False))
                                # syntax_content = Syntax(content_str, "json", theme="default", line_numbers=False)
                            elif isinstance(result.content, str) and (
                                    '\n' in result.content or result.content.strip().startswith('<')):
                                # Heuristic for code or XML/HTML
                                if event.tool_name == "ReadFileTool" and isinstance(event.result.message, str):
                                    # Try to guess lexer from file extension in message
                                    if ".py" in event.result.message:
                                        lexer = "python"
                                    elif ".js" in event.result.message:
                                        lexer = "javascript"
                                    elif ".ts" in event.result.message:
                                        lexer = "typescript"
                                    elif ".html" in event.result.message:
                                        lexer = "html"
                                    elif ".css" in event.result.message:
                                        lexer = "css"
                                    elif ".json" in event.result.message:
                                        lexer = "json"
                                    elif ".xml" in event.result.message:
                                        lexer = "xml"
                                    elif ".md" in event.result.message:
                                        lexer = "markdown"
                                    else:
                                        lexer = "text"  # Fallback lexer
                                elif event.tool_name == "ExecuteCommandTool":
                                    lexer = "shell"
                                else:
                                    lexer = "text"

                                content_str = _format_content(str(result.content))
                                # syntax_content = Syntax(
                                #     _format_content(result.content), lexer, theme="default", line_numbers=True
                                # )
                            else:
                                content_str = str(result.content)
                                # Append simple string content directly
                                panel_content.append(_format_content(content_str))

                        except Exception as e:
                            printer.print_text(f"Error formatting tool result content: {e}", style="yellow")
                            panel_content.append(
                                # Fallback
                                _format_content(str(result.content)))

                    # Print the base info panel
                    printer.print_panel(
                        content="\n".join(panel_content), title=title, border_style=border_style, center=True)
                    # Print syntax highlighted content separately if it exists
                    if content_str:
                        printer.print_code(
                            code=content_str, lexer=lexer, theme="monokai", line_numbers=True, panel=True)

                elif isinstance(event, PlanModeRespondEvent):
                    printer.print_panel(
                        content=Markdown(event.completion.response),
                        title="ğŸ ä»»åŠ¡å®Œæˆ", center=True
                    )

                elif isinstance(event, CompletionEvent):
                    # åœ¨è¿™é‡Œå®Œæˆå®é™…åˆå¹¶
                    # Ask æ¨¡å¼ä¸ä¼šå¯¹ä»£ç è¿›è¡Œå˜æ›´,æ•…æ”¾å¼ƒåˆå¹¶
                    # try:
                    #     self.apply_changes(request)
                    # except Exception as e:
                    #     printer.print_text(f"Error merging shadow changes to project: {e}", style="red")

                    printer.print_panel(
                        content=Markdown(event.completion.result),
                        title="ğŸ ä»»åŠ¡å®Œæˆ", center=True
                    )
                    if event.completion.command:
                        printer.print_text(f"Suggested command:{event.completion.command}", style="green")

                elif isinstance(event, ErrorEvent):
                    printer.print_panel(
                        content=f"Error: {event.message}",
                        title="ğŸ”¥ ä»»åŠ¡å¤±è´¥", center=True
                    )

                time.sleep(self.args.anti_quota_limit)  # Small delay for better visual flow

            # åœ¨å¤„ç†å®Œæ‰€æœ‰äº‹ä»¶åæ‰“å°ç´¯è®¡çš„tokenä½¿ç”¨æƒ…å†µ
            printer.print_key_value(accumulated_token_usage)

        except Exception as err:
            # åœ¨å¤„ç†å¼‚å¸¸æ—¶ä¹Ÿæ‰“å°ç´¯è®¡çš„tokenä½¿ç”¨æƒ…å†µ
            if accumulated_token_usage["input_tokens"] > 0:
                printer.print_key_value(accumulated_token_usage)
            printer.print_panel(content=f"FATAL ERROR: {err}", title="ğŸ”¥ Agentic Report è¿è¡Œé”™è¯¯", center=True)
            raise err

        printer.print_text("Agentic Report ç»“æŸ", style="green")