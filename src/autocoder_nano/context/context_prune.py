from copy import deepcopy
import json
import re
from typing import List, Tuple, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from pydantic import BaseModel

from autocoder_nano.actypes import SourceCode, VerifyFileRelevance, AutoCoderArgs
from autocoder_nano.core import prompt, extract_code, AutoLLM
from autocoder_nano.rag.token_counter import count_tokens
from autocoder_nano.utils.printer_utils import Printer
from autocoder_nano.utils.color_utils import *


printer = Printer()


class ContentPruner:

    def __init__(self, args: AutoCoderArgs, llm: AutoLLM, max_tokens: int):
        self.args = args
        self.llm = llm
        self.llm.setup_default_model_name(self.args.chat_model)
        self.max_tokens = max_tokens

    @staticmethod
    def _split_content_with_sliding_window(content: str, window_size=100, overlap=20) -> List[Tuple[int, int, str]]:
        """ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ†å‰²å¤§æ–‡ä»¶å†…å®¹ï¼Œè¿”å›åŒ…å«è¡Œå·ä¿¡æ¯çš„æ–‡æœ¬å—
        Args:
            content: è¦åˆ†å‰²çš„æ–‡ä»¶å†…å®¹
            window_size: æ¯ä¸ªçª—å£åŒ…å«çš„è¡Œæ•°
            overlap: ç›¸é‚»çª—å£çš„é‡å è¡Œæ•°
        Returns:
            List[Tuple[int, int, str]]: è¿”å›å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«:
                - èµ·å§‹è¡Œå·(ä»1å¼€å§‹)ï¼Œåœ¨åŸå§‹æ–‡ä»¶çš„ç»å¯¹è¡Œå·
                - ç»“æŸè¡Œå·ï¼Œåœ¨åŸå§‹æ–‡ä»¶çš„ç»å¯¹è¡Œå·
                - å¸¦è¡Œå·çš„å†…å®¹æ–‡æœ¬
        """
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.splitlines()
        chunks = []
        start = 0

        while start < len(lines):
            # è®¡ç®—å½“å‰çª—å£çš„ç»“æŸä½ç½®
            end = min(start + window_size, len(lines))

            # è®¡ç®—å®é™…çš„èµ·å§‹ä½ç½®(è€ƒè™‘é‡å )
            actual_start = max(0, start - overlap)

            # æå–å½“å‰çª—å£çš„è¡Œ
            chunk_lines = lines[actual_start:end]

            # ä¸ºæ¯ä¸€è¡Œæ·»åŠ è¡Œå·
            # è¡Œå·ä»actual_start+1å¼€å§‹ï¼Œä¿æŒä¸åŸæ–‡ä»¶çš„ç»å¯¹è¡Œå·ä¸€è‡´
            chunk_content = "\n".join([
                f"{i + 1} {line}" for i, line in enumerate(chunk_lines, start=actual_start)
            ])

            # ä¿å­˜åˆ†å—ä¿¡æ¯ï¼š(èµ·å§‹è¡Œå·, ç»“æŸè¡Œå·, å¸¦è¡Œå·çš„å†…å®¹)
            # è¡Œå·ä»1å¼€å§‹è®¡æ•°
            chunks.append((actual_start + 1, end, chunk_content))

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®
            # å‡å»overlapç¡®ä¿çª—å£é‡å 
            start += (window_size - overlap)

        return chunks

    def _delete_overflow_files(self, file_sources: List[SourceCode]) -> List[SourceCode]:
        """ ç›´æ¥åˆ é™¤è¶…å‡º token é™åˆ¶çš„æ–‡ä»¶ """
        total_tokens = 0
        selected_files = []
        # token_count = 0

        for file_source in file_sources:
            try:
                token_count = file_source.tokens
                if token_count <= 0:  # ç©ºæ–‡ä»¶ or å¼‚å¸¸æ–‡ä»¶
                    # token_count = 0
                    token_count = count_tokens(file_source.source_code)

                if total_tokens + token_count <= self.max_tokens:
                    total_tokens += token_count
                    selected_files.append(file_source)
                else:
                    break
            except Exception as e:
                printer.print_text(f"Failed to read file {file_source.module_name}: {e}", style=COLOR_ERROR)
                selected_files.append(file_source)

        return selected_files

    @prompt()
    def extract_code_snippets(
            self, conversations: List[Dict[str, str]], content: str, is_partial_content: bool = False
    ) -> str:
        """
        æ ¹æ®æä¾›çš„ä»£ç æ–‡ä»¶å’Œå¯¹è¯å†å²æå–ç›¸å…³ä»£ç ç‰‡æ®µã€‚

        å¤„ç†ç¤ºä¾‹ï¼š
        <examples>
        1.  ä»£ç æ–‡ä»¶ï¼š
        <code_file>
            1 def add(a, b):
            2     return a + b
            3 def sub(a, b):
            4     return a - b
        </code_file>
        <conversation_history>
            <user>: å¦‚ä½•å®ç°åŠ æ³•ï¼Ÿ
        </conversation_history>

        è¾“å‡ºï¼š
        ```json
        [
            {"start_line": 1, "end_line": 2}
        ]
        ```

        2.  ä»£ç æ–‡ä»¶ï¼š
            1 class User:
            2     def __init__(self, name):
            3         self.name = name
            4     def greet(self):
            5         return f"Hello, {self.name}"
        </code_file>
        <conversation_history>
            <user>: å¦‚ä½•åˆ›å»ºä¸€ä¸ªUserå¯¹è±¡ï¼Ÿ
        </conversation_history>

        è¾“å‡ºï¼š
        ```json
        [
            {"start_line": 1, "end_line": 3}
        ]
        ```

        3.  ä»£ç æ–‡ä»¶ï¼š
        <code_file>
            1 def foo():
            2     pass
        </code_file>
        <conversation_history>
            <user>: å¦‚ä½•å®ç°å‡æ³•ï¼Ÿ
        </conversation_history>

        è¾“å‡ºï¼š
        ```json
        []
        ```
        </examples>

        è¾“å…¥:
        1. ä»£ç æ–‡ä»¶å†…å®¹:
        <code_file>
        {{ content }}
        </code_file>

        <% if is_partial_content: %>
        <partial_content_process_note>
        å½“å‰å¤„ç†çš„æ˜¯æ–‡ä»¶çš„å±€éƒ¨å†…å®¹ï¼ˆè¡Œå·{start_line}-{end_line}ï¼‰ï¼Œ
        è¯·ä»…åŸºäºå½“å‰å¯è§å†…å®¹åˆ¤æ–­ç›¸å…³æ€§ï¼Œè¿”å›æ ‡æ³¨çš„è¡Œå·åŒºé—´ã€‚
        </partial_content_process_note>
        <% endif %>

        2. å¯¹è¯å†å²:
        <conversation_history>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </conversation_history>

        ä»»åŠ¡:
        1. åˆ†ææœ€åä¸€ä¸ªç”¨æˆ·é—®é¢˜åŠå…¶ä¸Šä¸‹æ–‡ã€‚
        2. åœ¨ä»£ç æ–‡ä»¶ä¸­æ‰¾å‡ºä¸é—®é¢˜ç›¸å…³çš„ä¸€ä¸ªæˆ–å¤šä¸ªé‡è¦ä»£ç æ®µã€‚
        3. å¯¹æ¯ä¸ªç›¸å…³ä»£ç æ®µï¼Œç¡®å®šå…¶èµ·å§‹è¡Œå·(start_line)å’Œç»“æŸè¡Œå·(end_line)ã€‚
        4. ä»£ç æ®µæ•°é‡ä¸è¶…è¿‡4ä¸ªã€‚

        è¾“å‡ºè¦æ±‚:
        1. è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«"start_line"å’Œ"end_line"ã€‚
        2. start_lineå’Œend_lineå¿…é¡»æ˜¯æ•´æ•°ï¼Œè¡¨ç¤ºä»£ç æ–‡ä»¶ä¸­çš„è¡Œå·ã€‚
        3. è¡Œå·ä»1å¼€å§‹è®¡æ•°ã€‚
        4. å¦‚æœæ²¡æœ‰ç›¸å…³ä»£ç æ®µï¼Œè¿”å›ç©ºæ•°ç»„[]ã€‚

        è¾“å‡ºæ ¼å¼:
        ä¸¥æ ¼çš„JSONæ•°ç»„ï¼Œä¸åŒ…å«å…¶ä»–æ–‡å­—æˆ–è§£é‡Šã€‚

        ```json
        [
            {"start_line": ç¬¬ä¸€ä¸ªä»£ç æ®µçš„èµ·å§‹è¡Œå·, "end_line": ç¬¬ä¸€ä¸ªä»£ç æ®µçš„ç»“æŸè¡Œå·},
            {"start_line": ç¬¬äºŒä¸ªä»£ç æ®µçš„èµ·å§‹è¡Œå·, "end_line": ç¬¬äºŒä¸ªä»£ç æ®µçš„ç»“æŸè¡Œå·}
        ]
        ```
        """

    def _extract_code_snippets(
            self, file_sources: List[SourceCode], conversations: List[Dict[str, str]]
    ) -> List[SourceCode]:
        """ æŠ½å–å…³é”®ä»£ç ç‰‡æ®µç­–ç•¥ """
        token_count = 0
        selected_files = []
        full_file_tokens = int(self.max_tokens * 0.8)

        total_input_tokens = sum(f.tokens for f in file_sources)
        printer.print_text(
            f"ğŸš€ å¼€å§‹ä»£ç ç‰‡æ®µæŠ½å–å¤„ç†ï¼Œå…± {len(file_sources)} ä¸ªæ–‡ä»¶ï¼Œæ€»tokenæ•°: {total_input_tokens}", style=COLOR_DEBUG
        )
        printer.print_text(
            f"ğŸ“‹ å¤„ç†ç­–ç•¥: å®Œæ•´æ–‡ä»¶ä¼˜å…ˆé˜ˆå€¼={full_file_tokens}, æœ€å¤§tokené™åˆ¶={self.max_tokens}", style=COLOR_DEBUG
        )

        for file_source in file_sources:
            try:
                # å®Œæ•´æ–‡ä»¶ä¼˜å…ˆ
                tokens = file_source.tokens
                if token_count + tokens <= full_file_tokens:
                    selected_files.append(SourceCode(
                        module_name=file_source.module_name, source_code=file_source.source_code, tokens=tokens))
                    token_count += tokens
                    printer.print_text(
                        f"âœ… æ–‡ä»¶ {file_source.module_name} å®Œæ•´ä¿ç•™ (tokenæ•°: {tokens}ï¼Œå½“å‰æ€»tokenæ•°: {token_count})",
                        style=COLOR_DEBUG
                    )
                    continue

                # å¦‚æœå•ä¸ªæ–‡ä»¶å¤ªå¤§ï¼Œé‚£ä¹ˆå…ˆæŒ‰æ»‘åŠ¨çª—å£åˆ†å‰²ï¼Œç„¶åå¯¹çª—å£æŠ½å–ä»£ç ç‰‡æ®µ
                if tokens > self.max_tokens:
                    chunks = self._split_content_with_sliding_window(
                        file_source.source_code,
                        self.args.context_prune_sliding_window_size,
                        self.args.context_prune_sliding_window_overlap
                    )
                    printer.print_text(
                        f"ğŸ“Š æ–‡ä»¶ {file_source.module_name} é€šè¿‡æ»‘åŠ¨çª—å£åˆ†å‰²ä¸º {len(chunks)} ä¸ªchunks", style=COLOR_DEBUG)

                    all_snippets = []
                    chunk_with_results = 0
                    for chunk_idx, (chunk_start, chunk_end, chunk_content) in enumerate(chunks):
                        printer.print_text(
                            f"ğŸ” å¤„ç†chunk {chunk_idx + 1}/{len(chunks)} (è¡Œå·: {chunk_start}-{chunk_end})",
                            style=COLOR_DEBUG)
                        extracted = self.extract_code_snippets.with_llm(self.llm).run(
                            conversations=conversations,
                            content=chunk_content,
                            is_partial_content=True
                        )
                        if extracted.output:
                            json_str = extract_code(extracted.output)[0][1]
                            snippets = json.loads(json_str)

                            if snippets:  # æœ‰æŠ½å–ç»“æœ
                                chunk_with_results += 1
                                printer.print_text(
                                    f"âœ… chunk {chunk_idx + 1} æŠ½å–åˆ° {len(snippets)} ä¸ªä»£ç ç‰‡æ®µ: {snippets}", style=COLOR_DEBUG)
                                # è·å–åˆ°çš„æœ¬æ¥å°±æ˜¯åœ¨åŸå§‹æ–‡ä»¶é‡Œçš„ç»å¯¹è¡Œå·
                                # åç»­åœ¨æ„å»ºä»£ç ç‰‡æ®µå†…å®¹æ—¶ï¼Œä¼šä¸ºäº†é€‚é…æ•°ç»„æ“ä½œä¿®æ”¹è¡Œå·ï¼Œè¿™é‡Œæ— éœ€å¤„ç†
                                adjusted_snippets = [{
                                    "start_line": snippet["start_line"],
                                    "end_line": snippet["end_line"]
                                } for snippet in snippets]
                                all_snippets.extend(adjusted_snippets)
                            else:
                                printer.print_text(f"âŒ chunk {chunk_idx + 1} æœªæŠ½å–åˆ°ç›¸å…³ä»£ç ç‰‡æ®µ", style=COLOR_ERROR)
                        else:
                            printer.print_text(f"âŒ chunk {chunk_idx + 1} æŠ½å–å¤±è´¥ï¼Œæœªè¿”å›ç»“æœ", style=COLOR_ERROR)
                    printer.print_text(
                        f"ğŸ“ˆ æ»‘åŠ¨çª—å£å¤„ç†å®Œæˆ: {chunk_with_results}/{len(chunks)} ä¸ªchunksæœ‰æŠ½å–ç»“æœï¼Œå…±æ”¶é›†åˆ° {len(all_snippets)} ä¸ªä»£ç ç‰‡æ®µ",
                        style=COLOR_DEBUG
                    )

                    merged_snippets = self._merge_overlapping_snippets(all_snippets)

                    printer.print_text(f"ğŸ”„ åˆå¹¶é‡å ç‰‡æ®µ: {len(all_snippets)} -> {len(merged_snippets)} ä¸ªç‰‡æ®µ",
                                       style=COLOR_DEBUG)
                    # if merged_snippets:
                    #     self.printer.print_str_in_terminal(f"    åˆå¹¶åçš„ç‰‡æ®µ: {merged_snippets}")

                    # åªæœ‰å½“æœ‰ä»£ç ç‰‡æ®µæ—¶æ‰å¤„ç†
                    if merged_snippets:
                        content_snippets = self._build_snippet_content(
                            file_source.module_name, file_source.source_code, merged_snippets)
                        snippet_tokens = count_tokens(content_snippets)

                        if token_count + snippet_tokens <= self.max_tokens:
                            selected_files.append(SourceCode(
                                module_name=file_source.module_name, source_code=content_snippets,
                                tokens=snippet_tokens))
                            token_count += snippet_tokens
                            printer.print_text(f"âœ… æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†æˆåŠŸï¼Œæœ€ç»ˆæŠ½å–åˆ°ç»“æœ", style=COLOR_DEBUG)
                            continue
                        else:
                            printer.print_text(
                                f"âŒ æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†åtokenæ•°è¶…é™"
                                f" ({token_count + snippet_tokens} > {self.max_tokens})ï¼Œåœæ­¢å¤„ç†",
                                style=COLOR_ERROR
                            )
                            break
                    else:
                        printer.print_text(
                            f"â­ï¸ æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†åæ— ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡å¤„ç†", style=COLOR_WARNING)
                        continue

                # æŠ½å–å…³é”®ç‰‡æ®µ
                lines = file_source.source_code.splitlines()
                new_content = ""

                # å°†æ–‡ä»¶å†…å®¹æŒ‰è¡Œç¼–å·
                for index, line in enumerate(lines):
                    new_content += f"{index + 1} {line}\n"

                printer.print_text(f"ğŸ” å¼€å§‹å¯¹æ–‡ä»¶ {file_source.module_name} è¿›è¡Œæ•´ä½“ä»£ç ç‰‡æ®µæŠ½å– (å…± {len(lines)} è¡Œ)",
                                   style=COLOR_DEBUG)

                extracted = self.extract_code_snippets.with_llm(self.llm).run(
                    conversations=conversations,
                    content=new_content
                )

                # æ„å»ºä»£ç ç‰‡æ®µå†…å®¹
                if extracted.output:
                    json_str = extract_code(extracted.output)[0][1]
                    snippets = json.loads(json_str)

                    if snippets:
                        printer.print_text(f"âœ… æŠ½å–åˆ° {len(snippets)} ä¸ªä»£ç ç‰‡æ®µ: {snippets}", style=COLOR_DEBUG)
                    else:
                        printer.print_text(f"âŒ æœªæŠ½å–åˆ°ç›¸å…³ä»£ç ç‰‡æ®µ", style=COLOR_ERROR)

                    # åªæœ‰å½“æœ‰ä»£ç ç‰‡æ®µæ—¶æ‰å¤„ç†
                    if snippets:
                        content_snippets = self._build_snippet_content(
                            file_source.module_name, file_source.source_code, snippets)
                        snippet_tokens = count_tokens(content_snippets)
                        if token_count + snippet_tokens <= self.max_tokens:
                            selected_files.append(SourceCode(module_name=file_source.module_name,
                                                             source_code=content_snippets,
                                                             tokens=snippet_tokens))
                            token_count += snippet_tokens
                            printer.print_text(f"âœ… æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–æˆåŠŸï¼Œæœ€ç»ˆæŠ½å–åˆ°ç»“æœ", style=COLOR_DEBUG)
                        else:
                            printer.print_text(
                                f"âŒ æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–åtokenæ•°è¶…é™"
                                f" ({token_count + snippet_tokens} > {self.max_tokens})ï¼Œåœæ­¢å¤„ç†",
                                style=COLOR_ERROR)
                            break
                    else:
                        # æ²¡æœ‰ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡è¿™ä¸ªæ–‡ä»¶
                        printer.print_text(f"â­ï¸ æ–‡ä»¶ {file_source.module_name} æ— ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡å¤„ç†", style=COLOR_WARNING)
                else:
                    printer.print_text(f"âŒ æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–å¤±è´¥ï¼Œæœªè¿”å›ç»“æœ", style=COLOR_ERROR)

            except Exception as e:
                printer.print_text(f"âŒ æ–‡ä»¶ {file_source.module_name} å¤„ç†å¼‚å¸¸: {e}", style=COLOR_ERROR)
                continue

        total_input_tokens = sum(f.tokens for f in file_sources)
        final_tokens = sum(f.tokens for f in selected_files)
        complete_files = 0
        snippet_files = 0
        for i, file_source in enumerate(file_sources):
            if i < len(selected_files):
                if selected_files[i].source_code == file_source.source_code:
                    complete_files += 1
                else:
                    snippet_files += 1

        printer.print_text(f"ğŸ¯ ä»£ç ç‰‡æ®µæŠ½å–å¤„ç†å®Œæˆ", style=COLOR_DEBUG)
        printer.print_text(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:", style=COLOR_DEBUG)
        printer.print_key_value(
            items={
                "è¾“å…¥æ–‡ä»¶æ•°": f"{len(file_sources)} ä¸ª",
                "è¾“å…¥tokenæ•°": f"{total_input_tokens}",
                "è¾“å‡ºæ–‡ä»¶æ•°": f"{len(selected_files)} ä¸ª",
                "è¾“å‡ºtokenæ•°": f"{final_tokens}",
                "Tokenå‹ç¼©ç‡": f"{((total_input_tokens - final_tokens) / total_input_tokens * 100):.1f}%",
                "å®Œæ•´ä¿ç•™æ–‡ä»¶": f"{complete_files} ä¸ª",
                "ç‰‡æ®µæŠ½å–æ–‡ä»¶": f"{snippet_files} ä¸ª",
                "è·³è¿‡å¤„ç†æ–‡ä»¶": f"{len(file_sources) - len(selected_files)} ä¸ª"
            }
        )
        return selected_files

    @staticmethod
    def _merge_overlapping_snippets(snippets: List[dict]) -> List[dict]:
        if not snippets:
            return []

        # æŒ‰èµ·å§‹è¡Œæ’åº
        sorted_snippets = sorted(snippets, key=lambda x: x["start_line"])

        merged = [sorted_snippets[0]]
        for current in sorted_snippets[1:]:
            last = merged[-1]
            if current["start_line"] <= last["end_line"] + 1:  # å…è®¸1è¡Œé—´éš”
                # åˆå¹¶åŒºé—´
                merged[-1] = {
                    "start_line": min(last["start_line"], current["start_line"]),
                    "end_line": max(last["end_line"], current["end_line"])
                }
            else:
                merged.append(current)

        return merged

    @staticmethod
    def _build_snippet_content(file_path: str, full_content: str, snippets: List[dict]) -> str:
        """æ„å»ºåŒ…å«ä»£ç ç‰‡æ®µçš„æ–‡ä»¶å†…å®¹"""
        lines = full_content.splitlines()
        header = f"Snippets:\n"

        content = []
        for snippet in snippets:
            start = max(0, snippet["start_line"] - 1)
            end = min(len(lines), snippet["end_line"])
            content.append(
                f"# Lines {start + 1}-{end} ({snippet.get('reason', '')})")
            content.extend(lines[start:end])

        return header + "\n".join(content)

    def prune(
            self, file_sources: List[SourceCode], conversations: List[Dict[str, str]], strategy: str = "score"
    ) -> List[SourceCode]:
        """
        å¤„ç†è¶…å‡º token é™åˆ¶çš„æ–‡ä»¶
        :param file_sources: è¦å¤„ç†çš„æ–‡ä»¶
        :param conversations: å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæå–ç­–ç•¥ï¼‰
        :param strategy: å¤„ç†ç­–ç•¥ (delete/extract/score)
            * `score`ï¼šé€šè¿‡å¯¹è¯åˆ†æå¯¹æ–‡ä»¶è¿›è¡Œç›¸å…³æ€§è¯„åˆ†ï¼Œä¿ç•™åˆ†æ•°æœ€é«˜çš„æ–‡ä»¶
            * `delete`ï¼šç®€å•åœ°ä»åˆ—è¡¨å¼€å§‹ç§»é™¤æ–‡ä»¶ï¼Œç›´åˆ°æ»¡è¶³ token é™åˆ¶
            * `extract`ï¼šåŸºäºç”¨æˆ·å¯¹è¯å†…å®¹æ™ºèƒ½æå–æ¯ä¸ªæ–‡ä»¶ä¸­çš„å…³é”®ä»£ç ç‰‡æ®µï¼Œæ˜¯å¤„ç†å¤§æ–‡ä»¶çš„æ¨èæ–¹å¼
        """
        file_paths = [file_source.module_name for file_source in file_sources]
        total_tokens, sources = self._count_tokens(file_sources=file_sources)
        if total_tokens <= self.max_tokens:
            return sources

        if strategy == "score":
            return self._score_and_filter_files(sources, conversations)
        if strategy == "delete":
            return self._delete_overflow_files(sources)
        elif strategy == "extract":
            return self._extract_code_snippets(sources, conversations)
        else:
            raise ValueError(f"æ— æ•ˆç­–ç•¥: {strategy}. å¯é€‰å€¼: delete/extract/score")

    @staticmethod
    def _count_tokens(file_sources: List[SourceCode]) -> Tuple[int, List[SourceCode]]:
        """è®¡ç®—æ–‡ä»¶æ€»tokenæ•°"""
        total_tokens = 0
        sources = []
        for file_source in file_sources:
            try:
                if file_source.tokens > 0:
                    tokens = file_source.tokens
                    total_tokens += file_source.tokens
                else:
                    tokens = count_tokens(file_source.source_code)
                    total_tokens += tokens

                sources.append(SourceCode(module_name=file_source.module_name,
                                          source_code=file_source.source_code, tokens=tokens))

            except Exception as e:
                printer.print_text(f"Failed to count tokens for {file_source.module_name}: {e}", style=COLOR_ERROR)
                sources.append(SourceCode(module_name=file_source.module_name,
                                          source_code=file_source.source_code, tokens=0))
        return total_tokens, sources

    @prompt()
    def verify_file_relevance(self, file_content: str, conversations: List[Dict[str, str]]) -> str:
        """
        è¯·éªŒè¯ä¸‹é¢çš„æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸ç”¨æˆ·å¯¹è¯ç›¸å…³:

        æ–‡ä»¶å†…å®¹:
        {{ file_content }}

        å†å²å¯¹è¯:
        <conversation_history>
        {% for msg in conversations %}
        <{{ msg.role }}>: {{ msg.content }}
        {% endfor %}
        </conversation_history>

        ç›¸å…³æ˜¯æŒ‡ï¼Œéœ€è¦ä¾èµ–è¿™ä¸ªæ–‡ä»¶æä¾›ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…éœ€è¦ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶æ‰èƒ½è§£å†³ç”¨æˆ·çš„é—®é¢˜ã€‚
        è¯·ç»™å‡ºç›¸åº”çš„å¯èƒ½æ€§åˆ†æ•°ï¼š0-10ï¼Œå¹¶ç»“åˆç”¨æˆ·é—®é¢˜ï¼Œç†ç”±æ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚æ ¼å¼å¦‚ä¸‹:

        ```json
        {
            "relevant_score": 0-10,
            "reason": "è¿™æ˜¯ç›¸å…³çš„åŸå› ï¼ˆä¸è¶…è¿‡10ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰..."
        }
        ```
        """

    def _score_and_filter_files(
            self, file_sources: List[SourceCode], conversations: List[Dict[str, str]]
    ) -> List[SourceCode]:
        """æ ¹æ®æ–‡ä»¶ç›¸å…³æ€§è¯„åˆ†è¿‡æ»¤æ–‡ä»¶ï¼Œç›´åˆ°tokenæ•°å¤§äºmax_tokens åœæ­¢è¿½åŠ """
        selected_files = []
        total_tokens = 0
        scored_files = []

        def _score_file(file_source: SourceCode) -> dict:
            try:
                score_result = self.verify_file_relevance.with_llm(self.llm).with_return_type(VerifyFileRelevance).run(
                    file_content=file_source.source_code,
                    conversations=conversations
                )
                print(score_result)
                return {
                    "file_path": file_source.module_name,
                    "score": score_result.relevant_score,
                    "tokens": file_source.tokens,
                    "content": file_source.source_code
                }
            except Exception as e:
                printer.print_text(f"Failed to score file {file_source.module_name}: {e}")
                return {}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰“åˆ†
        with ThreadPoolExecutor() as executor:
            futures = [executor.submit(_score_file, file_source) for file_source in file_sources]
            for future in as_completed(futures):
                result = future.result()
                if result:
                    scored_files.append(result)

        # ç¬¬äºŒæ­¥ï¼šæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        scored_files.sort(key=lambda x: x["score"], reverse=True)

        # ç¬¬ä¸‰æ­¥ï¼šä»é«˜åˆ†å¼€å§‹è¿‡æ»¤ï¼Œç›´åˆ°tokenæ•°å¤§äºmax_tokens åœæ­¢è¿½åŠ 
        for file_info in scored_files:
            if total_tokens + file_info["tokens"] <= self.max_tokens:
                selected_files.append(SourceCode(
                    module_name=file_info["file_path"],
                    source_code=file_info["content"],
                    tokens=file_info["tokens"]
                ))
                total_tokens += file_info["tokens"]
            else:
                break

        return selected_files


class PruneStrategy(BaseModel):
    name: str
    description: str
    config: Dict[str, Any] = {"safe_zone_tokens": 0}


class ConversationsPruner:
    def __init__(self, args: AutoCoderArgs, llm: AutoLLM):
        self.args = args
        self.llm = llm
        self.llm.setup_default_model_name(self.args.chat_model)
        self.replacement_message = ("This message has been cleared. If you still want to get this information, "
                                    "you can call the tool again to retrieve it.")
        self.strategies = {
            "tool_output_cleanup": PruneStrategy(
                name="tool_output_cleanup",
                description="å ä½è£å‰ªç­–ç•¥, é€šè¿‡ç”¨å ä½æ¶ˆæ¯æ›¿æ¢å†…å®¹æ¥æ¸…ç†å·¥å…·è¾“å‡ºç»“æœ",
                config={"safe_zone_tokens": self.args.conversation_prune_safe_zone_tokens}
            ),
            "summarize": PruneStrategy(
                name="summarize",
                description="æ‘˜è¦è£å‰ªç­–ç•¥, å¯¹æ—©æœŸå¯¹è¯è¿›è¡Œåˆ†ç»„æ‘˜è¦, ä¿ç•™å…³é”®ä¿¡æ¯",
                config={"safe_zone_tokens": self.args.conversation_prune_safe_zone_tokens,
                        "group_size": self.args.conversation_prune_group_size}
            ),
            "truncate": PruneStrategy(
                name="truncate",
                description="æˆªæ–­è£å‰ªç­–ç•¥, åˆ†ç»„æˆªæ–­æœ€æ—©çš„éƒ¨åˆ†å¯¹è¯",
                config={"safe_zone_tokens": self.args.conversation_prune_safe_zone_tokens,
                        "group_size": self.args.conversation_prune_group_size}
            ),
            "hybrid": PruneStrategy(
                name="hybrid",
                description="æ··åˆè£å‰ªç­–ç•¥, æ ¹æ®å¯¹è¯åˆ—è¡¨æƒ…å†µ, ç»„åˆä½¿ç”¨ä¸åŒç­–ç•¥",
                config={"safe_zone_tokens": self.args.conversation_prune_safe_zone_tokens,
                        "group_size": self.args.conversation_prune_group_size}
            )
        }

    @staticmethod
    def _split_system_messages(history_conversation):
        """ å¿«é€Ÿå°† conversation åˆ—è¡¨åˆ‡åˆ†ä¸º system å’Œ user+assistant ä¸¤ä¸ªåˆ—è¡¨ """
        split_index = next(
            (i for i, msg in enumerate(history_conversation) if msg["role"] != "system"),
            len(history_conversation)  # å¦‚æœå…¨æ˜¯systemæ¶ˆæ¯ï¼Œåˆ™è¿”å›æ•´ä¸ªé•¿åº¦
        )
        return history_conversation[:split_index], history_conversation[split_index:]

    def get_available_strategies(self) -> List[Dict[str, Any]]:
        """ è·å–æ‰€æœ‰å¯ç”¨ç­–ç•¥ """
        return [strategy.model_dump() for strategy in self.strategies.values()]

    def prune_conversations(
            self, conversations: List[Dict[str, Any]], strategy_name: str = "tool_output_cleanup"
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ç­–ç•¥ä¿®å‰ªå¯¹è¯
        Args:
            conversations: åŸå§‹å¯¹è¯åˆ—è¡¨
            strategy_name: ç­–ç•¥åç§°
        Returns:
            ä¿®å‰ªåçš„å¯¹è¯åˆ—è¡¨
        """
        safe_zone_tokens = self.args.conversation_prune_safe_zone_tokens
        current_tokens = count_tokens(json.dumps(conversations, ensure_ascii=False))

        if current_tokens <= safe_zone_tokens:
            return conversations

        strategy = self.strategies.get(self.args.conversation_prune_strategy, self.strategies["tool_output_cleanup"])

        if strategy.name == "tool_output_cleanup":
            return self._tool_output_cleanup_prune(conversations, strategy.config)
        elif strategy.name == "summarize":
            return self._summarize_prune(conversations, strategy.config)
        elif strategy.name == "truncate":
            return self._truncate_prune(conversations, strategy.config)
        elif strategy.name == "hybrid":
            return self._hybrid_prune(conversations, strategy.config)
        else:
            printer.print_text(f"æœªçŸ¥ç­–ç•¥ï¼š{strategy_name}ï¼Œå·²é»˜è®¤ä½¿ç”¨å ä½ç­–ç•¥", style=COLOR_WARNING)
            return self._tool_output_cleanup_prune(conversations, strategy.config)

    def _hybrid_prune(self, conversations: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ æ··åˆè£å‰ªç­–ç•¥ """
        safe_zone_tokens = config.get("safe_zone_tokens", 50 * 1024)
        group_size = config.get("group_size", 4)
        current_tokens = count_tokens(json.dumps(conversations, ensure_ascii=False))

        # æ··åˆè£å‰ªç­–ç•¥
        # 1. å¦‚æœä¼šè¯é•¿åº¦<10,è¿˜è¶…è¿‡safe_zone_tokens,è¯´æ˜å•ä¸ªä¼šè¯è¶…å¤§,æ­¤æ—¶å¦‚æœä½¿ç”¨å ä½å’Œæˆªæ–­å°†å¯¼è‡´é‡è¦ä¿¡æ¯ä¸¢å¤±,æ•…é‡‡ç”¨æ‘˜è¦ç­–ç•¥
        # 2. å¦‚æœä¼šè¯é•¿åº¦å¤„äº 11 - 50 ä¹‹é—´,è¯´æ˜è¿™æ˜¯ä¸€ä¸ªåˆšè¿è¡Œä¸ä¹…çš„agent,ä»¥è€ƒè™‘ç›´æ¥ä½¿ç”¨å ä½ç­–ç•¥
        # 3. å¦‚æœä¼šè¯é•¿åº¦å¤„äº 51 - 100 ä¹‹é—´,è¯´æ˜è¿™ä¸ªagentå·²ç»è¿è¡Œäº†æ¯”è¾ƒä¹…æˆ–è€…æ˜¯è·‘äº†å¤šè½®çš„agent,é€šè¿‡ä½¿ç”¨å ä½å’Œæ‘˜è¦ç»“åˆä½¿ç”¨
        # 4. å¦‚æœä¼šè¯é•¿åº¦ >100,è¯´æ˜è¿™æ˜¯ä¸€ä¸ªè¿è¡Œäº†è¶…é•¿æ—¶é—´çš„agent,é€šè¿‡ä½¿ç”¨å ä½,æ‘˜è¦å’Œæˆªæ–­ç»“åˆä½¿ç”¨

        if len(conversations) <= 10:  # æ‘˜è¦
            return self._summarize_prune(conversations,
                                         config={
                                             "safe_zone_tokens": self.args.conversation_prune_safe_zone_tokens,
                                             "group_size": 2  # ä½¿ç”¨ç‹¬ç«‹çš„config
                                         })
        elif 11 <= len(conversations) <= 50:  # å ä½
            return self._tool_output_cleanup_prune(conversations, config=config)
        elif 51 <= len(conversations) <= 100:  # æ‘˜è¦+å ä½
            summarized = self._summarize_prune(conversations,
                                               config={
                                                   "safe_zone_tokens": int(current_tokens * 0.8),
                                                   "group_size": self.args.conversation_prune_group_size
                                               })
            summarized_tokens = count_tokens(json.dumps(summarized, ensure_ascii=False))
            if summarized_tokens > self.args.conversation_prune_safe_zone_tokens:
                return self._tool_output_cleanup_prune(summarized, config=config)
            return summarized
        else:  # æˆªæ–­+æ‘˜è¦+å ä½
            truncated = self._truncate_prune(conversations,
                                             config={
                                                 "safe_zone_tokens": int(current_tokens * 0.8),
                                                 "group_size": self.args.conversation_prune_group_size
                                             })
            truncated_tokens = count_tokens(json.dumps(truncated, ensure_ascii=False))
            if truncated_tokens > self.args.conversation_prune_safe_zone_tokens:
                summarized = self._summarize_prune(truncated,
                                                   config={
                                                       "safe_zone_tokens": int(truncated_tokens * 0.8),
                                                       "group_size": self.args.conversation_prune_group_size
                                                   })
                summarized_tokens = count_tokens(json.dumps(summarized, ensure_ascii=False))
                if summarized_tokens > self.args.conversation_prune_safe_zone_tokens:
                    return self._tool_output_cleanup_prune(summarized, config=config)
                return summarized
            return truncated

    def _truncate_prune(self, conversations: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æˆªæ–­å¼å‰ªæ"""
        safe_zone_tokens = config.get("safe_zone_tokens", 50 * 1024)
        group_size = config.get("group_size", 4)
        processed_conversations = conversations.copy()

        system_conversations, other_conversations = self._split_system_messages(processed_conversations)

        init_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
        printer.print_text(f"[æˆªæ–­è£å‰ªç­–ç•¥]å¯¹è¯: {len(system_conversations + other_conversations)} æ¡, "
                           f"Tokenè®¡æ•°: {init_tokens}",
                           style=COLOR_DEBUG)
        while True:
            current_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
            if current_tokens <= safe_zone_tokens:
                printer.print_text(f"Tokenè®¡æ•°ï¼ˆ{current_tokens}ï¼‰å·²åœ¨å®‰å…¨åŒºï¼ˆ{safe_zone_tokens}ï¼‰å†…ï¼Œåœæ­¢è£å‰ª",
                                   style=COLOR_DEBUG)
                break

            # å¦‚æœå‰©ä½™å¯¹è¯ä¸è¶³ä¸€ç»„ï¼Œç›´æ¥è¿”å›ç³»ç»Ÿæç¤ºè¯åˆ—è¡¨
            if len(other_conversations) <= group_size:
                return system_conversations

            # ç§»é™¤æœ€æ—©çš„ä¸€ç»„å¯¹è¯
            other_conversations = other_conversations[group_size:]

        final_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
        printer.print_text(f"[æˆªæ–­è£å‰ªç­–ç•¥]æ¸…ç†å®Œæˆ, Tokenè®¡æ•°ï¼š{init_tokens} â†’ {final_tokens}", style=COLOR_DEBUG)

        return system_conversations + other_conversations

    def _summarize_prune(self, conversations: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ æ‘˜è¦å¼å‰ªæ """
        safe_zone_tokens = config.get("safe_zone_tokens", 50 * 1024)
        group_size = config.get("group_size", 4)
        processed_conversations = conversations.copy()

        system_conversations, other_conversations = self._split_system_messages(processed_conversations)

        init_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
        printer.print_text(f"[æ‘˜è¦è£å‰ªç­–ç•¥]å¯¹è¯: {len(system_conversations + other_conversations)} æ¡, "
                           f"Tokenè®¡æ•°: {init_tokens}",
                           style=COLOR_DEBUG)
        while True:
            current_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
            if current_tokens <= safe_zone_tokens:
                printer.print_text(f"Tokenè®¡æ•°ï¼ˆ{current_tokens}ï¼‰å·²åœ¨å®‰å…¨åŒºï¼ˆ{safe_zone_tokens}ï¼‰å†…ï¼Œåœæ­¢è£å‰ª",
                                   style=COLOR_DEBUG)
                break

            # æ‰¾åˆ°è¦å¤„ç†çš„å¯¹è¯ç»„
            early_conversations = other_conversations[:group_size]
            recent_conversations = other_conversations[group_size:]

            if not early_conversations:
                break

            # ç”Ÿæˆå½“å‰ç»„çš„æ‘˜è¦
            group_summary = self._generate_summary.with_llm(self.llm).run(
                conversations=early_conversations[-group_size:]
            )

            # æ›´æ–°å¯¹è¯å†å²
            other_conversations = [
                                       {"role": "user", "content": f"å†å²å¯¹è¯æ‘˜è¦ï¼š\n{group_summary.output}"},
                                       {"role": "assistant", "content": f"æ”¶åˆ°"}
                                   ] + recent_conversations

        final_tokens = count_tokens(json.dumps(system_conversations + other_conversations, ensure_ascii=False))
        printer.print_text(f"[æ‘˜è¦è£å‰ªç­–ç•¥]æ¸…ç†å®Œæˆ, Tokenè®¡æ•°ï¼š{init_tokens} â†’ {final_tokens}", style=COLOR_DEBUG)
        return system_conversations + other_conversations

    @prompt()
    def _generate_summary(self, conversations: List[Dict[str, Any]]) -> str:
        """
        è¯·ç”¨ä¸­æ–‡å°†ä»¥ä¸‹å¯¹è¯æµ“ç¼©ä¸ºè¦ç‚¹, ä¿ç•™å…³é”®å†³ç­–å’ŒæŠ€æœ¯ç»†èŠ‚, æµ“ç¼©è¦ç‚¹å­—æ•°è¦æ±‚ä¸ºåŸæ–‡çš„ 30% å·¦å³ï¼š

        <history_conversations>
        {{conversations}}
        </history_conversations>
        """

    def _tool_output_cleanup_prune(self, conversations: List[Dict[str, Any]], config: Dict[str, Any]
                                   ) -> List[Dict[str, Any]]:
        """
        é€šè¿‡ç”¨å ä½æ¶ˆæ¯æ›¿æ¢å†…å®¹æ¥æ¸…ç†å·¥å…·è¾“å‡ºç»“æœ
        è¯¥æ–¹æ³•æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        1. è¯†åˆ«å·¥å…·ç»“æœæ¶ˆæ¯ï¼ˆè§’è‰²ä¸º'user'ä¸”å†…å®¹åŒ…å«'<tool_result'çš„æ¶ˆæ¯ï¼‰
        2. ä»é¦–ä¸ªå·¥å…·è¾“å‡ºå¼€å§‹ä¾æ¬¡æ¸…ç†
        3. å½“tokenè®¡æ•°è¿›å…¥å®‰å…¨åŒºæ—¶åœæ­¢å¤„ç†
        """
        safe_zone_tokens = config.get("safe_zone_tokens", 50 * 1024)
        processed_conversations = conversations.copy()

        # æŸ¥æ‰¾æ‰€æœ‰å·¥å…·ç»“æœæ¶ˆæ¯çš„ç´¢å¼•
        tool_result_indices = []
        for i, conv in enumerate(processed_conversations):
            if conv.get("role") == "user" and isinstance(conv.get("content"), str) and self._is_tool_result_message(conv.get("content", "")):
                tool_result_indices.append(i)

        printer.print_text(f"[å ä½è£å‰ªç­–ç•¥]å‘ç° {len(tool_result_indices)} æ¡å¯èƒ½éœ€è¦æ¸…ç†çš„å·¥å…·ç»“æœæ¶ˆæ¯", style=COLOR_DEBUG)

        # ä¾æ¬¡æ¸…ç†å·¥å…·è¾“å‡ºï¼Œä»é¦–ä¸ªè¾“å‡ºå¼€å§‹
        init_tokens = count_tokens(json.dumps(processed_conversations, ensure_ascii=False))
        for tool_index in tool_result_indices:
            current_tokens = count_tokens(json.dumps(processed_conversations, ensure_ascii=False))

            if current_tokens <= safe_zone_tokens:
                printer.print_text(f"Tokenè®¡æ•°ï¼ˆ{current_tokens}ï¼‰å·²åœ¨å®‰å…¨åŒºï¼ˆ{safe_zone_tokens}ï¼‰å†…ï¼Œåœæ­¢è£å‰ª", style=COLOR_DEBUG)
                break

            # æå–å·¥å…·åç§°ä»¥ç”Ÿæˆæ›´å…·ä½“çš„æ›¿æ¢æ¶ˆæ¯
            tool_name = self._extract_tool_name(processed_conversations[tool_index]["content"])
            if tool_name in ["RecordMemoryTool"]:
                printer.print_text(f"[å ä½è£å‰ªç­–ç•¥]å·²è·³è¿‡æ¸…ç†ç´¢å¼•[{tool_index}]çš„å·¥å…·ç»“æœ({tool_name})")
            else:
                replacement_content = self._generate_replacement_message(tool_name)

                # æ›¿æ¢å†…å®¹
                original_content = processed_conversations[tool_index]["content"]
                processed_conversations[tool_index]["content"] = replacement_content

                printer.print_text(
                    f"[å ä½è£å‰ªç­–ç•¥]å·²æ¸…ç†ç´¢å¼•[{tool_index}]çš„å·¥å…·ç»“æœ({tool_name}),å­—ç¬¦æ•°ä» {len(original_content)} å‡å°‘åˆ° {len(replacement_content)}",
                    style=COLOR_DEBUG
                )

        final_tokens = count_tokens(json.dumps(processed_conversations, ensure_ascii=False))
        printer.print_text(f"[å ä½è£å‰ªç­–ç•¥]æ¸…ç†å®Œæˆã€‚Tokenè®¡æ•°ï¼š{init_tokens} â†’ {final_tokens}", style=COLOR_DEBUG)

        return processed_conversations

    @staticmethod
    def _is_tool_result_message(content: str) -> bool:
        """
        æ£€æŸ¥æ¶ˆæ¯å†…å®¹æ˜¯å¦åŒ…å«å·¥å…·ç»“æœ XML æ ¼å¼
        Args:
            content: å¾…æ£€æŸ¥çš„æ¶ˆæ¯å†…å®¹
        Returns:
            è‹¥å†…å®¹åŒ…å«å·¥å…·ç»“æœæ ¼å¼åˆ™è¿”å› True
        """
        return "<tool_result" in content and "tool_name=" in content

    @staticmethod
    def _extract_tool_name(content: str) -> str:
        """
        ä»å·¥å…·ç»“æœ XML å†…å®¹ä¸­è§£æå·¥å…·åç§°
        Args:
            content: å·¥å…·ç»“æœ XML å†…å®¹
        Returns:
            å·¥å…·åç§°ï¼Œè‹¥æœªæ‰¾åˆ°åˆ™è¿”å› 'unknown'
        """
        # Pattern to match: <tool_result tool_name='...' or <tool_result tool_name="..."
        pattern = r"<tool_result[^>]*tool_name=['\"]([^'\"]+)['\"]"
        match = re.search(pattern, content)
        if match:
            return match.group(1)
        return "unknown"

    def _generate_replacement_message(self, tool_name: str) -> str:
        """
        ç”Ÿæˆæ¸…ç†åçš„å·¥å…·ç»“æœæ›¿æ¢æ¶ˆæ¯
        Args:
            tool_name: è¢«è°ƒç”¨å·¥å…·çš„åç§°
        Returns:
            æ›¿æ¢æ¶ˆæ¯å­—ç¬¦ä¸²
        """
        if tool_name and tool_name != "unknown":
            return (f"<tool_result tool_name='{tool_name}' success='true'>"
                    f"<message>Content cleared to save tokens</message>"
                    f"<content>{self.replacement_message}</content>"
                    f"</tool_result>")
        else:
            return (f"<tool_result success='true'><message>[Content cleared to save tokens, you can call the tool "
                    f"again to get the tool result.]</message><"
                    f"content>{self.replacement_message}</content></tool_result>")

    def get_cleanup_statistics(self, original_conversations: List[Dict[str, Any]],
                               pruned_conversations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è·å–æ¸…ç†è¿‡ç¨‹çš„ç»Ÿè®¡ä¿¡æ¯
        Args:
            original_conversations: åŸå§‹å¯¹è¯åˆ—è¡¨
            pruned_conversations: æ¸…ç†åçš„å¯¹è¯åˆ—è¡¨
        Returns:
            åŒ…å«æ¸…ç†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        original_tokens = count_tokens(json.dumps(original_conversations, ensure_ascii=False))
        pruned_tokens = count_tokens(json.dumps(pruned_conversations, ensure_ascii=False))

        cleaned_count = 0
        for orig, pruned in zip(original_conversations, pruned_conversations):
            if (orig.get("role") == "user" and
                    self._is_tool_result_message(orig.get("content", "")) and
                    orig.get("content") != pruned.get("content")):
                cleaned_count += 1

        return {
            "original_tokens": original_tokens,
            "pruned_tokens": pruned_tokens,
            "tokens_saved": original_tokens - pruned_tokens,
            "compression_ratio": f"{(1 - pruned_tokens / original_tokens) * 100:.1f}%" if original_tokens > 0 else "0.0%",
            "tool_results_cleaned": cleaned_count,
            "total_messages": len(original_conversations)
        }
